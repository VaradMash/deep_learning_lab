{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "398cef54",
   "metadata": {},
   "source": [
    "<center><h1>Assignment 5</h1></center>\n",
    "\n",
    "## Problem Statement\n",
    "Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n",
    "1. Data preparation\n",
    "2. Generate training data\n",
    "3. Train model\n",
    "4. Output\n",
    "\n",
    "## Notebook Details\n",
    "1. Author : Varad Girish Mashalkar\n",
    "2. Branch : Information Technology\n",
    "3. Division : BE 11\n",
    "4. Batch : Q11\n",
    "5. Roll Number : 43335\n",
    "6. Course : Laboratory Practice 4 (Deep Learning)\n",
    "\n",
    "## Implementation Details\n",
    "1. Python version : 3.7.0\n",
    "2. Tensorflow version : 2.7.0 (Compatible with CUDA11.5 and cuDNN8.6.0)\n",
    "\n",
    "## Imports\n",
    "1. numpy\n",
    "2. tensorflow\n",
    "3. matplotlib\n",
    "4. gensim\n",
    "\n",
    "## Dataset link\n",
    "https://github.com/bhoomikamadhukar/NLP/blob/master/corona.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaabf4f",
   "metadata": {},
   "source": [
    "# 1. Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32183e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c2f86",
   "metadata": {},
   "source": [
    "# 2. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "899343a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset from text file\n",
    "raw_data = open(\"./corona.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac7f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_data = [text for text in raw_data if text.count(' ') >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54b012",
   "metadata": {},
   "source": [
    "# 3. Vectorizing the data using Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d39908",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = Tokenizer()\n",
    "vectorize.fit_on_texts(corona_data)\n",
    "corona_data = vectorize.texts_to_sequences(corona_data)\n",
    "total_vocab = sum(len(s) for s in corona_data)\n",
    "# total_vocab = 102\n",
    "word_count = len(vectorize.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f83d507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "print(total_vocab)\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743d780",
   "metadata": {},
   "source": [
    "# 4. Setting the window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94713af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328283e",
   "metadata": {},
   "source": [
    "# 5. Defining the CBOW model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5c5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining utility to generate context word pairs\n",
    "def cbow_model(data, window_size, total_vocab):\n",
    "    total_length = window_size*2\n",
    "    for text in data:\n",
    "        text_len = len(text)\n",
    "        for idx, word in enumerate(text):\n",
    "            context_word = []\n",
    "            target   = []            \n",
    "            begin = idx - window_size\n",
    "            end = idx + window_size + 1\n",
    "            context_word.append([\n",
    "                text[i] \n",
    "                for i in range(begin, end) \n",
    "                if 0 <= i < text_len \n",
    "                and i != idx\n",
    "            ])\n",
    "            target.append(word)\n",
    "            contextual = sequence.pad_sequences(\n",
    "                context_word, \n",
    "                maxlen=total_length\n",
    "            )\n",
    "            final_target = np_utils.to_categorical(\n",
    "                target, \n",
    "                total_vocab\n",
    "            )\n",
    "            yield(contextual, final_target)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a9f294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 14:06:11.723185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 14:06:11.750024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 14:06:11.750221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 14:06:11.750551: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-30 14:06:11.751559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 14:06:11.751767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 14:06:11.751918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 14:06:12.159541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 14:06:12.159736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 14:06:12.159894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 14:06:12.160038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2636 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=total_vocab, \n",
    "        output_dim=100, \n",
    "        input_length=window_size*2\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    Lambda(\n",
    "        lambda x: K.mean(x, axis=1), \n",
    "        output_shape=(100,)\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    Dense(\n",
    "        total_vocab, \n",
    "        activation='softmax'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa2a4f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 100)            19800     \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 198)               19998     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39,798\n",
      "Trainable params: 39,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Checking model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16d5b4",
   "metadata": {},
   "source": [
    "# 6. Compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e982e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07d1e9",
   "metadata": {},
   "source": [
    "# 7. Training the CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3904c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 \t:  1041.8004126548767\n",
      "Epoch  1 \t:  992.3414831161499\n",
      "Epoch  2 \t:  903.7286412715912\n",
      "Epoch  3 \t:  826.9192087650299\n",
      "Epoch  4 \t:  774.075676202774\n",
      "Epoch  5 \t:  723.7201819419861\n",
      "Epoch  6 \t:  670.4681974649429\n",
      "Epoch  7 \t:  615.2118247747421\n",
      "Epoch  8 \t:  559.9114753007889\n",
      "Epoch  9 \t:  506.38645058870316\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    cost = 0\n",
    "    for x, y in cbow_model(corona_data, window_size, total_vocab):\n",
    "        cost += model.train_on_batch(x, y)\n",
    "    print(\"Epoch \", i,\"\\t: \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405444e",
   "metadata": {},
   "source": [
    "# 8. Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b77be",
   "metadata": {},
   "source": [
    "### Writing vector to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d12ff58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions = 100\n",
    "vect_file = open('./vectors.txt' ,'w')\n",
    "vect_file.write('{} {}\\n'.format(102, dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffd31cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()[0]\n",
    "for text, i in vectorize.word_index.items():\n",
    "    final_vec = ' '.join(map(str, list(weights[i, :])))\n",
    "    vect_file.write('{} {}\\n'.format(text, final_vec))\n",
    "vect_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fbf9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_output = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'vectors.txt', \n",
    "    binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4da2c1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('interval', 0.7260262370109558),\n",
       " ('both', 0.7044325470924377),\n",
       " ('19', 0.6949691772460938),\n",
       " ('than', 0.6716287732124329),\n",
       " ('higher', 0.6118828058242798),\n",
       " ('before', 0.5684723258018494),\n",
       " ('shed', 0.5653639435768127),\n",
       " ('covid', 0.5485020279884338),\n",
       " ('for', 0.5267449021339417),\n",
       " ('speed', 0.4975537359714508)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_output.most_similar(positive=['virus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94040bc0",
   "metadata": {},
   "source": [
    "<center><h1>End of Notebook</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3.7_DL] *",
   "language": "python",
   "name": "conda-env-python3.7_DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
